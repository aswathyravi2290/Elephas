# -*- coding: utf-8 -*-
"""elephas_xray_tpu

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Vb8a0VuzImNxR-EAsaTv0GkpfenVe1Qq
"""

pip install elephas

from pyspark import SparkContext, SparkConf
conf = SparkConf().setAppName('Elephas_App').setMaster('local[8]')
sc = SparkContext(conf=conf)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Activation
from tensorflow.keras.optimizers import SGD
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Activation
from tensorflow.keras.utils import to_categorical
from tensorflow.keras import optimizers

from elephas.ml_model import ElephasEstimator
from elephas.ml.adapter import to_data_frame

from pyspark import SparkContext, SparkConf
from pyspark.mllib.evaluation import MulticlassMetrics
from pyspark.ml import Pipeline

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import tensorflow as tf
import keras.preprocessing.image
import sklearn.preprocessing
import sklearn.model_selection
import sklearn.metrics
import sklearn.linear_model
import sklearn.naive_bayes
import sklearn.tree
import sklearn.ensemble
import os;
import datetime  
import cv2 
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.cm as cm  
# %matplotlib inline



from google.colab import drive
drive.mount('/content/drive')

import cv2
from imutils import paths
import os
import time
from keras.preprocessing.image import img_to_array

data=[]
labels=[]
i=1
d=[]

imagePaths = sorted(list(paths.list_images("/content/drive/MyDrive/chest_xray/train")))
for imagePath in imagePaths:
  print(i)
  i+=1
  image = cv2.imread(imagePath)
  image = cv2.resize(image,(50, 50))
  image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
  image = img_to_array(image)
  label = imagePath.split(os.path.sep)[-2]
  if label=="NORMAL":
    label=1
    d.append((image,label))
    data.append(image)
    labels.append(label)
  elif label=="PNEUMONIA":
    label=0
    data.append(image)
    d.append((image,label))
    labels.append(label)

import random
random.shuffle(d)

data=[]
labels=[]
for i in d:
  data.append(i[0])
  labels.append(i[1])

import numpy as np
data = np.array(data, dtype="float") / 255.0
labels = np.array(labels)

train_X = data
train_y = labels
VAL_PCT = 0.1  # lets reserve 10% of our data for validation
val_size = int(len(data)*VAL_PCT)
print(val_size)

train_X = data[:-val_size]
train_y = labels[:-val_size]

test_X = data[-val_size:]
test_y = labels[-val_size:]

print(len(train_X),len(test_X))
d=0
n=0
for i in range(len(train_y)):
  if train_y[i]==1:
    n+=1
  else:
    d+=1
print(d,n)

EPOCHS = 30
BATCH_SIZE = 16
steps_per_epoch = len(train_X)//BATCH_SIZE
print("Steps per epoch: ", steps_per_epoch)

import tensorflow as tf

model = tf.keras.Sequential(
      [
        
        tf.keras.layers.Conv2D(32, (5, 5), input_shape = (50, 50, 1), activation = 'relu'), # no bias necessary before batch norm
        tf.keras.layers.MaxPooling2D(pool_size = (2, 2)),
        
        tf.keras.layers.Conv2D(64, (5, 5), activation = 'relu'),
        tf.keras.layers.MaxPooling2D(pool_size = (2, 2)),
        
        tf.keras.layers.Conv2D(128, (5, 5), activation = 'relu'),
        tf.keras.layers.MaxPooling2D(pool_size = (2, 2)),

        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(units = 512, activation = 'relu'),
        
        tf.keras.layers.Dense(1, activation='sigmoid')
      ])

from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Activation
from tensorflow.keras.utils import to_categorical
from tensorflow.keras import optimizers

from elephas.ml_model import ElephasEstimator
from elephas.ml.adapter import to_data_frame

from pyspark import SparkContext, SparkConf
from pyspark.mllib.evaluation import MulticlassMetrics
from pyspark.ml import Pipeline

"""batch size 16"""

start_time=time.time()
from elephas.spark_model import SparkModel
from elephas.utils.rdd_utils import to_simple_rdd
sgd = SGD(lr=0.1)
model.compile(sgd, 'categorical_crossentropy', ['acc'])

# Build RDD from numpy features and labels
rdd = to_simple_rdd(sc,train_X, train_y)

# Initialize SparkModel from tensorflow.keras model and Spark context
spark_model = SparkModel(model, mode='asynchronous')

# Train Spark model
spark_model.fit(rdd, epochs=30, batch_size=16, verbose=2, validation_split=0.1)

# Evaluate Spark model by evaluating the underlying model
score = spark_model.evaluate(test_X, test_y, verbose=2)
print('Test accuracy:', score[1])
print("--- %s seconds ---" % (time.time() - start_time))

start_time=time.time()
from elephas.spark_model import SparkModel
from elephas.utils.rdd_utils import to_simple_rdd
sgd = SGD(lr=0.1)
model.compile(sgd, 'categorical_crossentropy', ['acc'])

# Build RDD from numpy features and labels
rdd = to_simple_rdd(sc,train_X, train_y)

# Initialize SparkModel from tensorflow.keras model and Spark context
spark_model = SparkModel(model, mode='synchronous')

# Train Spark model
spark_model.fit(rdd, epochs=30, batch_size=16, verbose=2, validation_split=0.1)

# Evaluate Spark model by evaluating the underlying model
score = spark_model.evaluate(test_X, test_y, verbose=2)
print('Test accuracy:', score[1])
print("--- %s seconds ---" % (time.time() - start_time))

from tensorflow.keras.callbacks import *
start_time=time.time()
sgd = SGD(lr=0.1)
model.compile(sgd, 'categorical_crossentropy', ['acc'])
model.fit(train_X,train_y, steps_per_epoch=steps_per_epoch, epochs=EPOCHS,validation_data=(test_X,test_y),batch_size=BATCH_SIZE)
print("--- %s seconds ---" % (time.time() - start_time))

score = model.evaluate(test_X, test_y, verbose=2)
print('Test accuracy:', score[1])

from keras.utils.vis_utils import plot_model
plot_model(model,  show_shapes=True, show_layer_names=True)

"""batch size 32 """

start_time=time.time()
from elephas.spark_model import SparkModel
from elephas.utils.rdd_utils import to_simple_rdd
sgd = SGD(lr=0.1)
model.compile(sgd, 'categorical_crossentropy', ['acc'])

# Build RDD from numpy features and labels
rdd = to_simple_rdd(sc,train_X, train_y)

# Initialize SparkModel from tensorflow.keras model and Spark context
spark_model = SparkModel(model, mode='asynchronous')

# Train Spark model
spark_model.fit(rdd, epochs=30, batch_size=32, verbose=2, validation_split=0.1)

# Evaluate Spark model by evaluating the underlying model
score = spark_model.evaluate(test_X, test_y, verbose=2)
print('Test accuracy:', score[1])
print("--- %s seconds ---" % (time.time() - start_time))

start_time=time.time()
from elephas.spark_model import SparkModel
from elephas.utils.rdd_utils import to_simple_rdd
sgd = SGD(lr=0.1)
model.compile(sgd, 'categorical_crossentropy', ['acc'])

# Build RDD from numpy features and labels
rdd = to_simple_rdd(sc,train_X, train_y)

# Initialize SparkModel from tensorflow.keras model and Spark context
spark_model = SparkModel(model, mode='synchronous')

# Train Spark model
spark_model.fit(rdd, epochs=30, batch_size=32, verbose=2, validation_split=0.1)

# Evaluate Spark model by evaluating the underlying model
score = spark_model.evaluate(test_X, test_y, verbose=2)
print('Test accuracy:', score[1])
print("--- %s seconds ---" % (time.time() - start_time))

from tensorflow.keras.callbacks import *
start_time=time.time()
sgd = SGD(lr=0.1)
model.compile(sgd, 'categorical_crossentropy', ['acc'])
model.fit(train_X,train_y, steps_per_epoch=steps_per_epoch, epochs=EPOCHS,validation_data=(test_X,test_y),batch_size=32)
print("--- %s seconds ---" % (time.time() - start_time))